# kgxqr.github.io

## Experiments
- [RQ1](https://github.com/kgxqr/kgxqr.github.io/tree/main/RQ). We evaluate the intrinsic quality of the concept knowledge graph by assessing the correctness of the [concepts](https://github.com/kgxqr/kgxqr.github.io/blob/main/RQ/RQ2.Wiki_conceptions.xlsx) and [relations](https://github.com/kgxqr/kgxqr.github.io/blob/main/RQ/RQ1.SO_relations.xlsx) in the knowledge graph.

- RQ2. We evaluate the effectiveness of KGXQR for relevant question retrieval by comparing it with multiple baselines on different test [datasets](https://github.com/kgxqr/kgxqr.github.io/tree/main/dataset). We create a benchmark consisting of three test datasets with different characteristics. The AnswerBot dataset is human annotated small datasets and contain only Java-related queries, while duplicate question dataset and title edit dataset are automatically created large datasets and cover the four most popular programming languages. The detailed evaluation result is shown in [RQ2.xlsx](https://github.com/kgxqr/kgxqr.github.io/blob/main/RQ/RQ2.xlsx). The Stack Overflow dump used in our implementation includes 1,096,708 duplicate question records and 2,554,062 edit records for 16,663,358 questions. As a result, we sample 21,172 and 80,000 positive samples from the duplicate question records and the question edit records respectively.
- [RQ3](https://github.com/kgxqr/kgxqr.github.io/blob/main/RQ/RQ3.xlsx). To evaluate the usefulness of the explanations provided by KGXQR, we compare the performance of users in selecting relevant questions for specific programming tasks with the explanations generated by KGXQR and a baseline method respectively.

