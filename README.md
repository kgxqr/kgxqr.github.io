# kgxqr.github.io

## Experiments
- [RQ1](https://github.com/kgxqr/kgxqr.github.io/tree/main/RQ). We evaluate the intrinsic quality of the concept knowledge graph by assessing the correctness of the [concepts](https://github.com/kgxqr/kgxqr.github.io/blob/main/RQ/RQ2.Wiki_conceptions.xlsx) and [relations](https://github.com/kgxqr/kgxqr.github.io/blob/main/RQ/RQ1.SO_relations.xlsx) in the knowledge graph.

- RQ2. We evaluate the effectiveness of KGXQR for relevant question retrieval by comparing it with multiple baselines on different test [datasets](https://github.com/kgxqr/kgxqr.github.io/tree/main/dataset). We create a benchmark consisting of three test datasets with different characteristics. 
  - The [AnswerBot dataset](https://github.com/kgxqr/kgxqr.github.io/blob/main/dataset/answer_bot.txt) is human annotated small datasets and contain only Java-related queries, while duplicate question datasets of [java](https://github.com/kgxqr/kgxqr.github.io/blob/main/dataset/duplicate_java.txt)/[c#](https://github.com/kgxqr/kgxqr.github.io/blob/main/dataset/duplicate_c%23.txt)/[javascript](https://github.com/kgxqr/kgxqr.github.io/blob/main/dataset/duplicate_javascript.txt)/[python](https://github.com/kgxqr/kgxqr.github.io/blob/main/dataset/duplicate_python.txt) and title edit datasets of [java](https://github.com/kgxqr/kgxqr.github.io/blob/main/dataset/history_java.txt)/[c#](https://github.com/kgxqr/kgxqr.github.io/blob/main/dataset/history_c%23.txt)/[javascript](https://github.com/kgxqr/kgxqr.github.io/blob/main/dataset/history_javascript.txt)/[python](https://github.com/kgxqr/kgxqr.github.io/blob/main/dataset/history_python.txt) are automatically created large datasets and cover the four most popular programming languages. The detailed evaluation result is shown in [RQ2.xlsx](https://github.com/kgxqr/kgxqr.github.io/blob/main/RQ/RQ2.xlsx). And the corresponding document stores for [java](https://github.com/kgxqr/kgxqr.github.io/blob/main/dataset/document_store_java.json), [c#](https://github.com/kgxqr/kgxqr.github.io/blob/main/dataset/document_store_c%23.json), [javascript](https://github.com/kgxqr/kgxqr.github.io/blob/main/dataset/document_store_javascript.json) and [python](https://github.com/kgxqr/kgxqr.github.io/blob/main/dataset/document_store_python.json) are also listed in the dataset.
  - The Stack Overflow dump used in our implementation includes 1,096,708 duplicate question records and 2,554,062 edit records for 16,663,358 questions. As a result, we sample 21,172 and 80,000 positive samples from the duplicate question records and the question edit records respectively. The detailed information of [positive training data](https://github.com/kgxqr/kgxqr.github.io/blob/main/dataset/train_positive.txt) and corresponding [negative training data](https://github.com/kgxqr/kgxqr.github.io/blob/main/dataset/train_negative.txt) is shown in two files, [duplicate data file](https://github.com/kgxqr/kgxqr.github.io/blob/main/dataset/train_duplicate_data.txt) and [history data file](https://github.com/kgxqr/kgxqr.github.io/blob/main/dataset/train_history_data.txt).
- [RQ3](https://github.com/kgxqr/kgxqr.github.io/blob/main/RQ/RQ3.xlsx). To evaluate the usefulness of the explanations provided by KGXQR, we compare the performance of users in selecting relevant questions for specific programming tasks with the explanations generated by KGXQR and a baseline method respectively.

